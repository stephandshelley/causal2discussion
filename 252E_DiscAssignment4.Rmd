---
title: 'Discussion Assignment #4'
author: "Sabrina Boyce, Shelley Facente and Steph Holm"
date: "11/14/2019"
output: pdf_document
---

##Question 1: Consider the point treatment example presented in the paper. What assumptions are needed for identifiability? Why do we need the positivity assumption? Focus on the strong positivity assumption.
For identifiability in general, we must assume that all measured covariates are sufficient to control for confounding, and that there is sufficient variablity in treatment/exposure assigmment within strata of confounders (positivity). In the point treatment example presented in the paper, we also require the assumption that $A$ and $Y_a$ are conditionally independent given $W$ (this is the randomization assumption).

Specifically, the strong positivity assumption (assumption of experimental treatment assignment) can be written as:
$$
\underset{a\in \mathcal{A}}{\mathrm{inf}} \ g_0(a|W) > 0, \mathrm{-a.e.}
$$

This means there is some positivity probability of following each level of a given regime, given every set of Ws supported by the data generating process. 

We need this because the G-computation formula is only valid if the conditional distributions in the formula are well-defined; if one or more treatment levels of interest do not occur within some covariate strata then the conditional probability $P_0(Y=y|A=a,W=w)$ will not be well-defined for some value(s) $(a,w)$.

## Question 2: Consider the longitudinal example presented above. What are the analogous assumptions? Interpret the strong positivity assumption in words.
1. Sequential randomization assumption
$Y \perp A(t), \bar{L}(t), \bar{A}(t-1) = \bar{a}(t-1)), \ \ t = 1,...,K$

2. Positivity assumption
$P[\bar{A}(t) = \bar{a}(t), \bar{L}(t), \bar{A}(t-1) = \bar{a}(t-1)] >0, \ \ t = 1,...,K$

The strong positivity assumption in the logitudinal case means there is some positivity probability of following each level of a given regime at every timepoint, given every set of W and L(t)s for that timepoint, supported by the data generating process. 

## Question 3: How do theoretical and practical violations of the positivity assumption arise? Give a real world example of each. Do you think practical positivity violations are more or less likely to be a problem for longitudinal versus point treatment interventions? Why?
Theoretical violations of the positivity assumption arise when something is biologically implausible, such as in a case of cisgender males who would never receive a hysterectomy ($g(hysterectomy | cis male) = 0$). Practical positivity violations occur when a sample size is small or unbalanced such that when a sample is stratified on a confounder (or set of confounders), there is no variation in the treatment levels of interest. 

Practical positivity violations are more likely to be a problem for longitudinal interventions, because there are multiple treatment nodes that make up the overall treatment regime; therefore, there are many more opportunities for practical positivity violations to arise.

## Question 4: Focus on the point treatment example in the paper. How would you estimate the coefficients of a working MSM with a G computation estimator? Describe the impact of positivity violations on the performance of this estimator.
The coefficients of a working MSM are estimated by generating a predicted counterfactual outcome for each subject under each possible treatment:
$\hat{Y}_{a,i} = \bar{Q}_n(a,W_i) \text{ for }a \in \mathcal{A}, i = 1,...,n.$

\vspace{10pt}

The estimate $\hat{\beta}_{Gcomp}$ is then obtained by regressing $\hat{Y}_a$ on $a$ and $V$ according to the model $m(a,V|\beta)$, with weights based on the projection function $h(a,V)$.  
  
If there are positivity violations, some of the conditional probabilities are not defined. 
  
The estimator can extrapolate based on covariate strata where there are not sparsity issues.

* this depends heavily on the model for $\bar{Q}_0$
* if the model used to estimate $\bar{Q}_0$ is misspecified, the resulting effect estimates will be biased.


##Question 5: Focus on the point treatment example in the paper. How would you estimate the coefficients of a working MSM with IPTW? 

The coefficients of a working MSM can be estimated using a weighted regression of the outcome Y on A and W using the model $m(A,V|\beta)$ with weights $1/g_n(A|W)$  

$g_n(A|W)$ can be estimated using a model selection regime that uses cross-validation and loss-based learning.  

IPTW is especially sensitive to positivity violations: If in a finite sample there are few observations within certain strata combinations such that $g_n(A|W=w)$ approaches zero the following will likely result:  

* Weights on the rare individuals that did have this combination will be extreme   
* Bias in the estimate    
* Variance will be inflated   
 
If there are 0 individuals in the sample for a certain strata combination, the following will likely result: 
   
* Variance of the estimate will be reduced   
* Bias in the estimate can become extreme 
 

Weight truncation of extreme weights (ex. 1st and 99th percentile) is used to reduce variance and reduce the impact of a few rare individuals on the effect estimate. 

This, however, leads to misspecification of the treatment model $g_n$, which is a bias that we cannot easily correct.  

## Sabrina-Question 6: Provide an overview of the properties of the AIPTW and TMLE estimators. Describe impact of positivity violations on the performance of AIPTW and TMLE.

TMLE - TMLE relies on estimation of both $Q_0$ and $g_0$ and is consistent if either one is a consistent estimator AND satisfies the positivity assumption. 

When positivity assumption cannot be satisfied the estimate relies entirely on a consistent estimation of $Q_0$ and are then subject to the same vulnerabilies of the G-comp estimate (data extrapolation using a model that is likely misspecified). 

## Sabrina-Question 7: What are a few quick ways to diagnose positivity violations? What are some of their short-comings?

Some quick ways to diagnose positivity violations include:  

1. Descriptive statistics of treatment variability within covariate strata    
    **limitations: unweildy with large number of strata combinations, continuous or multilevel variables    
2. Examine the distribution of the weights  
    **limitations: if strata contain 0 observations, you will not see this weight in the distribution.     
3. Examine the distribution of the propensity score values ($g_n(a|W)$) for $a \in A$ to see if any are close to 0  
    **limitations: this can diagnose a problem but not quantify its severity  
 

## Steph- Question 8: Discuss the authors proposed parametric bootstrap.
### (a) Formally define bias for an estimator.

The bias is  the difference between the true value of the target parameter ($\Psi(P_0)$) and the expectation of the estimator from a sample of  n i.i.d. observations ($\hat{\psi} (P_n)$):

$$Bias(\hat{\Psi}, P_0, n) = E_{P_0}\hat{\psi} (P_n) -\Psi(P_0)$$


### (b) What are some of the causes of bias?

1.  inconsistency of the estimators $g_n$ or $Q_n$
2.  $g_0$ does not satisfy the positivity assumption
3.  $g_n$ or $Q_n$ may have large finite sample bias even if they are consistent
4.  estimated values of $g_n$ might be close to zero or one

### (c) Describe the parametric bootstrap-based biased estimate and its implementation.

The parametric bootstrap bias estimate uses as estimate ($\hat{P}_0$) of the observed data distribution ($P_0$), and samples a bootstrap distribution (${P_n}^\#$). This means that a sample of size n is resampled *with replacement* to draw many new samples of size n. The candidate estimator is applied to each bootstrapped sample and the mean of this is compared to the true value:

$\hat{Bias_{PB}}(\hat{\Psi}, \hat{P_0}, n) = E_{\hat{P_0}}\hat{\psi} ({P_n}^\#) -\Psi(\hat{P_0})$

### (d) What is the goal of the proposed algorithm? What are sources of bias does it help identify? What are some of its limitations?

The goal of the proposed algorithm is to not only diagnose positivity violations but also to estimate the quantity of the resulting bias. This does not provide an accurate estimate of the bias,  but does alow the investigator to assess when bias might be a threat to inference. It only identifies bias from positivity violations and near-violations. A limitation of this method is that  it does not address identifiability issues, ie. biases  that make the observed parameter different from the target causal parameter. It also will not diagnose inconsistency  in the estimators (point #1 in part b above).



## Steph- Question 9. Describe the following approaches to responding to positivity violations, using an example. Discuss their pros/cons.

### (a) Changing the projection function h(a; V )

You can change the projection function to only apply over a certain range of treatment levels. For example, if FDA approved treatment dosages are a smaller range than all dosages seen in the dataset, you could restrict to the FDA approved dosages. You can also write a function that will allow you to find the target parameter which minimizes the positivity violations within an allowable range of bias, and use bootstrapping to implement this. That function could be written as: $h_\delta (a, V) = I(a \in [c(\delta), d(\delta)])$, where increasing $\delta$ implies a progressive restriction on the allowable range of treatment levels. And advantage of this strategy is that it works well with a marginal structural model. Disadvantages include ...

### (b) Restricting the adjustment set

Another technique is to restrict levels of the covariates that cause positivity violations (or near violations). A disadvantage of this method is that things that cause positivity violations are often strong confounders so these exclusions may worsen identifiability as well. As described in part a, how much to restrict can also be determined using bootstrapping to balance restriction and bias.

### (c) Restricting the sample (trimming)

This can be done by eliminating subjects with particular values of W , but this can also happen by calculating propensity scores, and eliminating subjects with extreme propensity scores. There is a trade-off between the representativeness of the subsample in which things can be estimated and the variance of the estimate.

Disadvantages of this include minimizing sample size, creating new practical positivity violations as sample size decreases, and minimizing external validity to a subset of the population of interest.

### (d) Changing the intervention of interest

You can use dynamic treatment regimes to change the intervention of interest, so that the counterfactual treatments are more appropriate for the treatment levels seen in the observed data. For example, if people with a poor kidney function only ever get a lower dose of the medication,  kidney function could be included as a criterion in  a dynamic treatment  regime.


## Steph-Question 10. For each approach, consider whether it could be applied in the setting of a longitudinal intervention (in which control for time dependent confounding is required for identifiability). If not, why not (or under what conditions would it break down)? If so, how could it be used to mitigate threats due to positivity? Use an example to illustrate.

### (a) Changing the projection function h(a; V )

This would work well for the longitudinal setting.

### (b) Restricting the adjustment set

As in the point  treatment setting, restricting the adjustment set in longitudinal questions can increase the bias.

### (c) Restricting the sample (trimming)

This is problematic for longitudinal situations because the  covariates that cause positivity violations may be affected by past treatment, which can introduce new biases by conditioning on post-treatment covariates. 


### (d) Changing the intervention of interest

Dynamic regimes can work well in longitudinal settings.

## Sabrina-Question 11. How could you formalize the selection between different target parameters?

One could formalize the selection of a target parameter by: 
1. Selecting a family of target parameters and indexing them in some way by the degree to which each parameter trades improved identifiability for proximity to the target causal parameter
2. Define a threshold of an acceptable amount of bias
3. Using parametric bootstrapping to estimate the amount of bias for each parameter in the family
4. Select the target parameter that falls below the designated threshold for bias and offers the greatest proximity to the target causal parameter. 

-->