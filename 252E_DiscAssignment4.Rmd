---
title: 'Discussion Assignment #4'
author: "Sabrina Boyce, Shelley Facente and Steph Holm"
date: "11/14/2019"
output:
  beamer_presentation: default
---

## Shelley- Question 4: Focus on the point treatment example in the paper. How would you estimate the coefficients of a working MSM with a G computation estimator? Describe the impact of positivity violations on the performance of this estimator.

## Sabrina- Question 5: Focus on the point treatment example in the paper. How would you estimate the coefficients of a working MSM with IPTW? Describe the impact of positivity violations on the performance of IPTW. What is the impact of weight truncation?


## Shelley- Question 1: Consider the point treatment example presented in the paper. What assumptions are needed for identifiability? Why do we need the positivity assumption? Focus on the strong positivity assumption.

## Shelley- Question 2: Consider the longitudinal example presented above. What are the analogous assumptions? Interpret the strong positivity assumption in words.

## Shelley- Question 3: How do theoretical and practical violations of the positivity assumption arise? Give a real world example of each. Do you think practical positivity violations are more or less likely to be a problem for longitudinal versus point treatment interventions? Why?

## Sabrina-Question 6: Provide an overview of the properties of the AIPTW and TMLE estimators. Describe impact of positivity violations on the performance of AIPTW and TMLE.

## Sabrina-Question 7: What are a few quick ways to diagnose positivity violations? What are some of their short-comings?

## Steph- Question 8: Discuss the authors proposed parametric bootstrap.
### (a) Formally define bias for an estimator.

The bias is  the difference between the true value of the target parameter ($\Psi(P_0)$) and the expectation of the estimator from a sample of  n i.i.d. observations ($\hat{\psi} (P_n)$):

$$Bias(\hat{\Psi}, P_0, n) = E_{P_0}\hat{\psi} (P_n) -\Psi(P_0)$$


### (b) What are some of the causes of bias?

1.  inconsistency of the estimators $g_n$ or $Q_n$
2.  $g_0$ does not satisfy the positivity assumption
3.  $g_n$ or $Q_n$ may have large finite sample bias even if they are consistent
4.  estimated values of $g_n$ might be close to zero or one

### (c) Describe the parametric bootstrap-based biased estimate and its implementation.

The parametric bootstrap bias estimate uses as estimate ($\hat{P}_0$) of the observed data distribution ($P_0$), and samples a bootstrap distribution (${P_n}^\#$). This means that a sample of size n is resampled *with replacement* to draw many new samples of size n. The candidate estimator is applied to each bootstrapped sample and the mean of this is compared to the true value:

$\hat{Bias_{PB}}(\hat{\Psi}, \hat{P_0}, n) = E_{\hat{P_0}}\hat{\psi} ({P_n}^\#) -\Psi(\hat{P_0})$

### (d) What is the goal of the proposed algorithm? What are sources of bias does it help identify? What are some of its limitations?

The goal of the proposed algorithm is to not only diagnose positivity violations but also to estimate the quantity of the resulting bias. This does not provide an accurate estimate of the bias,  but does alow the investigator to assess when bias might be a threat to inference. It only identifies bias from positivity violations and near-violations. A limitation of this method is that  it does not address identifiability issues, ie. biases  that make the observed parameter different from the target causal parameter. It also will not diagnose inconsistency  in the estimators (point #1 in part b above).



## Steph- Question 9. Describe the following approaches to responding to positivity violations, using an example. Discuss their pros/cons.

### (a) Changing the projection function h(a; V )

You can change the projection function to only apply over a certain range of treatment levels. For example, if FDA approved treatment dosages are a smaller range than all dosages seen in the dataset, you could restrict to the FDA approved dosages. You can also write a function that will allow you to find the target parameter which minimizes the positivity violations within an allowable range of bias, and use bootstrapping to implement this. That function could be written as: $h_\delta (a, V) = I(a \in [c(\delta), d(\delta)])$, where increasing $\delta$ implies a progressive restriction on the allowable range of treatment levels. And advantage of this strategy is that it works well with a marginal structural model. Disadvantages include ...

### (b) Restricting the adjustment set

Another technique is to restrict levels of the covariates that cause positivity violations (or near violations). A disadvantage of this method is that things that cause positivity violations are often strong confounders so these exclusions may worsen identifiability as well. As described in part a, how much to restrict can also be determined using bootstrapping to balance restriction and bias.

### (c) Restricting the sample (trimming)

This can be done by eliminating subjects with particular values of W , but this can also happen by calculating propensity scores, and eliminating subjects with extreme propensity scores. There is a trade-off between the representativeness of the subsample in which things can be estimated and the variance of the estimate.

Disadvantages of this include minimizing sample size, creating new practical positivity violations as sample size decreases, and minimizing external validity to a subset of the population of interest.

### (d) Changing the intervention of interest

You can use dynamic treatment regimes to change the intervention of interest, so that the counterfactual treatments are more appropriate for the treatment levels seen in the observed data. For example, if people with a poor kidney function only ever get a lower dose of the medication,  kidney function could be included as a criterion in  a dynamic treatment  regime.


## Steph-Question 10. For each approach, consider whether it could be applied in the setting of a longitudinal intervention (in which control for time dependent confounding is required for identifiability). If not, why not (or under what conditions would it break down)? If so, how could it be used to mitigate threats due to positivity? Use an example to illustrate.

### (a) Changing the projection function h(a; V )

This would work well for the longitudinal setting.

### (b) Restricting the adjustment set

As in the point  treatment setting, restricting the adjustment set in longitudinal questions can increase the bias.

### (c) Restricting the sample (trimming)

This is problematic for longitudinal situations because the  covariates that cause positivity violations may be affected by past treatment, which can introduce new biases by conditioning on post-treatment covariates. 


### (d) Changing the intervention of interest

Dynamic regimes can work well in longitudinal settings.

## Sabrina-Question 11. How could you formalize the selection between different target parameters?
-->